{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "331800e0-41c0-402f-9d35-06c20453f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate(dataset):\n",
    "    img, (rel_questions, rel_answers), (norel_questions, norel_answers) = dataset\n",
    "    colors = ['red ', 'green ', 'blue ', 'orange ', 'gray ', 'yellow ']\n",
    "    answer_sheet = ['yes', 'no', 'rectangle', 'circle', '1', '2', '3', '4', '5', '6']\n",
    "    questions = rel_questions + norel_questions\n",
    "    answers = rel_answers + norel_answers\n",
    "\n",
    "    print(rel_questions)\n",
    "    print(rel_answers)\n",
    "\n",
    "\n",
    "    for question,answer in zip(questions,answers):\n",
    "        query = ''\n",
    "        query += colors[question.tolist()[0:6].index(1)]\n",
    "\n",
    "        if question[6] == 1:\n",
    "            if question[8] == 1:\n",
    "                query += 'shape?'\n",
    "            if question[9] == 1:\n",
    "                query += 'left?'\n",
    "            if question[10] == 1:\n",
    "                query += 'up?'\n",
    "        if question[7] == 1:\n",
    "            if question[8] == 1:\n",
    "                query += 'closest shape?'\n",
    "            if question[9] == 1:\n",
    "                query += 'furthest shape?'\n",
    "            if question[10] == 1:\n",
    "                query += 'count?'\n",
    "\n",
    "        ans = answer_sheet[answer]\n",
    "        print(query,'==>', ans)\n",
    "    #cv2.imwrite('sample.jpg',(img*255).astype(np.int32))\n",
    "    cv2.imshow('img',cv2.resize(img,(512,512)))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "def decode_question(questions, answers):\n",
    "    for question,answer in zip(questions,answers):\n",
    "        query = ''\n",
    "        query += colors[question.tolist()[0:6].index(1)]\n",
    "        \n",
    "        # non-relational question\n",
    "        if question[NOREL_Q_IDX] == 1:\n",
    "            if question[SUB_Q_IDX] == 1:\n",
    "                query += 'shape?'\n",
    "            if question[SUB_Q_IDX + 1] == 1:\n",
    "                query += 'left?'\n",
    "            if question[SUB_Q_IDX + 2] == 1:\n",
    "                query += 'up?'\n",
    "                \n",
    "        # relational question\n",
    "        if question[REL_Q_IDX] == 1:\n",
    "            if question[SUB_Q_IDX] == 1:\n",
    "                query += 'closest shape?'\n",
    "            if question[SUB_Q_IDX + 1] == 1:\n",
    "                query += 'furthest shape?'\n",
    "            if question[SUB_Q_IDX + 2] == 1:\n",
    "                query += 'count?'\n",
    "\n",
    "        ans = answer_sheet[answer]\n",
    "        print(query,'==>', ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1c4e2-ae84-4f9f-8853-924289ab1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import argparse\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c467cc-ba8c-4c94-955a-fe5a7599c7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd903d9-866a-4133-aa8b-510488394519",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"relational-networks/data/sort-of-clevr.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c816254-efa4-44f3-9b4e-0ba536d043e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9800, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]), len(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e8eda88-3a71-4123-b8e7-bfdaba57c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('img', cv2.resize(data[0][0][0], (512, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c581e339-a49f-46d9-b923-e73945af8575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('sample1.jpg',(data[0][0][0]*255).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86eab2a-cdc1-42b0-9ecb-e64ae273de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('img', data[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd4733d7-9b37-4fde-ba29-7a98bb961806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cf5ec80-de8c-49a9-a823-63028054387c",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(cv2.cvtColor(cv2.resize(data[0][0][0], (512, 512)), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c09245e8-dc74-431b-bf69-e432c770f4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.resize(data[0][0][0], (512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a852d2-2b9a-4ebd-9d8f-ebd8a0b92976",
   "metadata": {},
   "source": [
    "Although neural networks like CNNs and MLPs are powerful in a variety of tasks, they struggle with relational reasoning. Relation network is a neural network module designed for relational reasoning tasks. It constrains the functional form of NN to avoid \n",
    "\n",
    "Relation Networks (RN) can be simply represented by equation below, where $O$ is a set of objects, denoted as $O = \\{o_1, o_2, ..., o_n\\}$. Meanwhile, $f_{\\phi}$ and $g_{\\theta}$ are MLPs with learnable and differentiable parameters $\\phi$ and $\\theta$.\n",
    "\n",
    "$$RN(O) = f_{\\phi} \\Big( \\sum_{i, j} g_{\\theta} (o_i, o_j)\\Big)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "49ecc544-834c-4f22-9208-1fa21b6f6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvInputModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvInputModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.conv1(img)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm4(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380d222-08f6-414f-a3c7-7a78d3334bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b209949-504d-4586-8305-4f5eb2453a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b697799-486b-4af0-bee5-42b111831801",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "085a009b-249e-4bdd-9019-b588c8a4916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvInputModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "53cae189-c468-40a3-b0f4-b050953c9ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 75, 75])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3de82fb1-ce6f-4d34-abf0-ac515e4b4d5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[129], line 27\u001b[0m, in \u001b[0;36mConvInputModel.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(img)\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatchNorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:410\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_input_dim\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 4D input (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n",
      "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "model(torch.Tensor(data[0]).permute(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2e07a-db3e-4acc-9333-eddc3766057e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "caf9e1d7-1787-49c0-a429-44583d348d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e4d559a-14c9-4b0f-9d1a-ca92336d8370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ./data already exists\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "TRAIN_SIZE, TEST_SIZE = 9800, 200\n",
    "IMAGE_SIZE, SYMBOL_SIZE = 75, 5\n",
    "QUESTION_ENCODING_SIZE = 11  ## 6 for one-hot vector of color, 2 for question type (rel or norel), 3 for question subtype\n",
    "NOREL_Q_IDX, REL_Q_IDX = 6, 7\n",
    "SUB_Q_IDX = 8\n",
    "ANSWER_ENCODING = {\n",
    "    'no': 0, 'yes': 1, 'rectangle': 2, 'circle': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9\n",
    "}\n",
    "COLORS_LIST = ['red ', 'green ', 'blue ', 'orange ', 'gray ', 'yellow ']\n",
    "ANSWERS_LIST = ['no', 'yes', 'rectangle', 'circle', '1', '2', '3', '4', '5', '6']\n",
    "\n",
    "nb_questions = 10\n",
    "dirs = './data'\n",
    "\n",
    "COLORS = [\n",
    "    (0,0,255),##r\n",
    "    (0,255,0),##g\n",
    "    (255,0,0),##b\n",
    "    (0,156,255),##o\n",
    "    (128,128,128),##k\n",
    "    (0,255,255)##y\n",
    "]\n",
    "\n",
    "try:\n",
    "    os.makedirs(dirs)\n",
    "except:\n",
    "    print('directory {} already exists'.format(dirs))\n",
    "\n",
    "def generate_new_center(objects):\n",
    "    while True:\n",
    "        valid_new_center = True\n",
    "        center = np.random.randint(0 + SYMBOL_SIZE, IMAGE_SIZE - SYMBOL_SIZE, 2)        \n",
    "        if len(objects) > 0:\n",
    "            for color, c, shape in objects:\n",
    "                if ((center - c) ** 2).sum() < ((SYMBOL_SIZE * 2) ** 2):\n",
    "                    valid_new_center = False\n",
    "        if valid_new_center:\n",
    "            return center\n",
    "        \n",
    "def generate_new_object(objects, color_id, img):\n",
    "    # generate a new valid center\n",
    "    center = generate_new_center(objects)\n",
    "    # sample a shape from rectangle or circle of equal probabiliy\n",
    "    if random.random() < 0.5:\n",
    "        start = (center[0] - size, center[1] - size)\n",
    "        end = (center[0] + size, center[1] + size)\n",
    "        cv2.rectangle(img, start, end, COLORS[color_id], -1)\n",
    "        objects.append((color_id, center, 'rectangle'))\n",
    "    \n",
    "    else:\n",
    "        center_ = (center[0], center[1])\n",
    "        cv2.circle(img, center_, size, COLORS[color_id], -1)\n",
    "        objects.append((color_id, center, 'circle'))\n",
    "        \n",
    "def generate_non_relational_question(objects):\n",
    "    question = np.zeros((QUESTION_ENCODING_SIZE))\n",
    "    color_id = random.randint(0,5)\n",
    "    question[color_id] = 1\n",
    "    question[NOREL_Q_IDX] = 1\n",
    "    subtype = random.randint(0,2)\n",
    "    question[subtype + SUB_Q_IDX] = 1\n",
    "    \n",
    "    if subtype == 0:\n",
    "        # query shape -> rectangle/circle \n",
    "        answer = ANSWER_DICT[objects[color_id][2]]\n",
    "    elif subtype == 1: \n",
    "        # query horizontal position is on the left -> yes/no\n",
    "        horizontal_left = (objects[color_id][1][0] < IMAGE_SIZE / 2).astype(int)\n",
    "        answer = 1 - horizontal_left\n",
    "    elif subtype == 2:\n",
    "        # query vertical position is on the top -> yes/no\n",
    "        vertical_top = (objects[color_id][1][1] < IMAGE_SIZE / 2).astype(int)\n",
    "        answer = 1 - vertical_top\n",
    "        \n",
    "    return question, answer \n",
    "\n",
    "def generate_relational_question(objects):\n",
    "    question = np.zeros((QUESTION_ENCODING_SIZE))\n",
    "    color_id = random.randint(0,5)\n",
    "    question[color_id] = 1\n",
    "    question[REL_Q_IDX] = 1\n",
    "    subtype = random.randint(0,2)\n",
    "    question[subtype + SUB_Q_IDX] = 1\n",
    "    \n",
    "    if subtype in [0, 1]:\n",
    "        # query closest-to or furthest-from -> rectangle / circle\n",
    "        curr_obj = objects[color_id][1]\n",
    "        dist_list = [((curr_obj - obj[1]) ** 2).sum() for obj in objects]\n",
    "        dist_list[dist_list.index(0)] = math.inf\n",
    "        closest_ind = dist_list.index(min(dist_list))\n",
    "        furthest_ind = dist_list.index(max(dist_list))\n",
    "        if subtype == 0:\n",
    "            answer = ANSWER_DICT[objects[closest_ind][2]]\n",
    "        else:\n",
    "            answer = ANSWER_DICT[objects[furthest_ind][2]]\n",
    "\n",
    "    elif subtype == 2:\n",
    "        \"\"\"count->1~6\"\"\"\n",
    "        curr_obj = objects[color_id][2]\n",
    "        count = -1\n",
    "        for obj in objects:\n",
    "            if obj[2] == curr_obj:\n",
    "                count += 1 \n",
    "        answer = count + 4\n",
    "        \n",
    "    return question, answer \n",
    "\n",
    "def sample_one_image():\n",
    "    \n",
    "    objects = []\n",
    "    rel_questions, rel_answers = [], []\n",
    "    norel_questions, norel_answers = [], []\n",
    "    \n",
    "    # Getting a white image canvas\n",
    "    img = np.ones((IMAGE_SIZE, IMAGE_SIZE, 3)) * 255\n",
    "    \n",
    "    # Adding 6 objects, one color each\n",
    "    for color_id, color in enumerate(COLORS):  \n",
    "        generate_new_object(objects, color_id, img)\n",
    "    \n",
    "    # Sample questions \n",
    "    for _ in range(nb_questions):\n",
    "        norel_question, norel_answer = generate_non_relational_question(objects)\n",
    "        rel_question, rel_answer = generate_relational_question(objects)\n",
    "        norel_questions.append(norel_question)\n",
    "        norel_answers.append(norel_answer)\n",
    "        rel_questions.append(rel_question)\n",
    "        rel_answers.append(rel_answer)\n",
    "    \n",
    "    relations = (rel_questions, rel_answers)\n",
    "    norelations = (norel_questions, norel_answers)\n",
    "    \n",
    "    img = img/255.\n",
    "    dataset = (img, relations, norelations)\n",
    "    return dataset, objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "af3bf091-aeda-40a1-aef6-9efab20b79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = []\n",
    "for _ in range(TEST_SIZE):\n",
    "    one_data, one_obj = sample_one_image()\n",
    "    img_data.append(one_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5f1019f5-8fcf-4d91-915a-e193f59727f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.array(img_data, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9056c50c-0569-4382-bd7f-e44b186a8a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 256, 5, 5])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor(img_data).permute(0,3,1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9296160f-cb6a-4a4f-96d2-669544a21676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([17, 25]), 'rectangle'),\n",
       " (1, array([22, 42]), 'rectangle'),\n",
       " (2, array([13, 47]), 'circle'),\n",
       " (3, array([66, 62]), 'rectangle'),\n",
       " (4, array([53, 35]), 'circle'),\n",
       " (5, array([28, 28]), 'rectangle')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "74201507-41d7-40b1-93fb-dce413204d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow left? ==> no\n",
      "red shape? ==> rectangle\n",
      "gray up? ==> no\n",
      "red up? ==> yes\n",
      "green up? ==> yes\n",
      "yellow left? ==> no\n",
      "gray shape? ==> rectangle\n",
      "red up? ==> yes\n",
      "yellow up? ==> no\n",
      "gray up? ==> no\n"
     ]
    }
   ],
   "source": [
    "decode_question(*data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "16f4db94-b931-4845-8f39-e05cd9b501df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('sample1new.jpg',(data[0]*255).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e4785c6c-a38b-4aaf-8daf-545f072e58a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([ 6, 17]), 'rectangle'),\n",
       " (1, array([44, 24]), 'rectangle'),\n",
       " (2, array([60, 64]), 'circle'),\n",
       " (3, array([ 7, 38]), 'circle'),\n",
       " (4, array([49, 58]), 'rectangle'),\n",
       " (5, array([42, 67]), 'circle')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f70e5cc7-0bba-4ad8-bd09-2db3c380affa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdffdd23ac0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(data[0])\n",
    "plt.imshow((data[0] * 255).astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
